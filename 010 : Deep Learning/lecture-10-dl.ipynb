{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8380280,"sourceType":"datasetVersion","datasetId":4983414}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Alif Firmansyah\n# 1103204105\n# Lecture 10 : Boston Housing Dataset Using PyTorch\n# Dataset : Boston House Prices, Model : Neural Network, PyTorch\n# Dokumentasi ChatGPT : https://chat.openai.com/share/f646d3c2-d763-4fb8-92b0-a3755690adc3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T05:18:11.983245Z","iopub.execute_input":"2024-05-11T05:18:11.983658Z","iopub.status.idle":"2024-05-11T05:18:11.989038Z","shell.execute_reply.started":"2024-05-11T05:18:11.983627Z","shell.execute_reply":"2024-05-11T05:18:11.987692Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"# 1. Memuat Library","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:15.321049Z","iopub.execute_input":"2024-05-11T05:18:15.321477Z","iopub.status.idle":"2024-05-11T05:18:15.327622Z","shell.execute_reply.started":"2024-05-11T05:18:15.321447Z","shell.execute_reply":"2024-05-11T05:18:15.326304Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"# 2. Menelusuri Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/regression-house/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/regression-house/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:16.555668Z","iopub.execute_input":"2024-05-11T05:18:16.556110Z","iopub.status.idle":"2024-05-11T05:18:16.607106Z","shell.execute_reply.started":"2024-05-11T05:18:16.556074Z","shell.execute_reply":"2024-05-11T05:18:16.605859Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:16.710985Z","iopub.execute_input":"2024-05-11T05:18:16.711445Z","iopub.status.idle":"2024-05-11T05:18:16.737880Z","shell.execute_reply.started":"2024-05-11T05:18:16.711406Z","shell.execute_reply":"2024-05-11T05:18:16.736932Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n\n  YrSold  SaleType  SaleCondition  SalePrice  \n0   2008        WD         Normal     208500  \n1   2007        WD         Normal     181500  \n2   2008        WD         Normal     223500  \n3   2006        WD        Abnorml     140000  \n4   2008        WD         Normal     250000  \n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 81 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Separate features and target variable\nX_train = train_df.drop(columns=['SalePrice']) \ny_train = train_df['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:16.861107Z","iopub.execute_input":"2024-05-11T05:18:16.861546Z","iopub.status.idle":"2024-05-11T05:18:16.869796Z","shell.execute_reply.started":"2024-05-11T05:18:16.861513Z","shell.execute_reply":"2024-05-11T05:18:16.868292Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# One-hot encode categorical variables\nX_train = pd.get_dummies(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:17.002521Z","iopub.execute_input":"2024-05-11T05:18:17.002946Z","iopub.status.idle":"2024-05-11T05:18:17.053877Z","shell.execute_reply.started":"2024-05-11T05:18:17.002913Z","shell.execute_reply":"2024-05-11T05:18:17.052524Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:17.135329Z","iopub.execute_input":"2024-05-11T05:18:17.135787Z","iopub.status.idle":"2024-05-11T05:18:17.181581Z","shell.execute_reply.started":"2024-05-11T05:18:17.135742Z","shell.execute_reply":"2024-05-11T05:18:17.180515Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:17.288542Z","iopub.execute_input":"2024-05-11T05:18:17.289204Z","iopub.status.idle":"2024-05-11T05:18:17.295126Z","shell.execute_reply.started":"2024-05-11T05:18:17.289155Z","shell.execute_reply":"2024-05-11T05:18:17.294092Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"# 3. Modelling","metadata":{}},{"cell_type":"code","source":"# Define the neural network model\nclass RegressionModel(nn.Module):\n    def __init__(self, input_size):\n        super(RegressionModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:17.452202Z","iopub.execute_input":"2024-05-11T05:18:17.452636Z","iopub.status.idle":"2024-05-11T05:18:17.460504Z","shell.execute_reply.started":"2024-05-11T05:18:17.452600Z","shell.execute_reply":"2024-05-11T05:18:17.459536Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\ninput_size = X_train.shape[1]\nmodel = RegressionModel(input_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:18.207620Z","iopub.execute_input":"2024-05-11T05:18:18.208057Z","iopub.status.idle":"2024-05-11T05:18:18.214424Z","shell.execute_reply.started":"2024-05-11T05:18:18.208024Z","shell.execute_reply":"2024-05-11T05:18:18.213499Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:18.634909Z","iopub.execute_input":"2024-05-11T05:18:18.635357Z","iopub.status.idle":"2024-05-11T05:18:18.642149Z","shell.execute_reply.started":"2024-05-11T05:18:18.635319Z","shell.execute_reply":"2024-05-11T05:18:18.640571Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluasi","metadata":{}},{"cell_type":"code","source":"# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T05:18:30.680938Z","iopub.execute_input":"2024-05-11T05:18:30.681386Z","iopub.status.idle":"2024-05-11T05:18:31.026754Z","shell.execute_reply.started":"2024-05-11T05:18:30.681352Z","shell.execute_reply":"2024-05-11T05:18:31.025766Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"Epoch [10/100], Loss: nan\nEpoch [20/100], Loss: nan\nEpoch [30/100], Loss: nan\nEpoch [40/100], Loss: nan\nEpoch [50/100], Loss: nan\nEpoch [60/100], Loss: nan\nEpoch [70/100], Loss: nan\nEpoch [80/100], Loss: nan\nEpoch [90/100], Loss: nan\nEpoch [100/100], Loss: nan\n","output_type":"stream"}]}]}